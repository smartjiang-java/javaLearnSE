# Table of Contents

  * [1：一条sql查询语句是如何执行的？](#1一条sql查询语句是如何执行的)
  * [2：一条sql更新语句是如何执行的？](#2一条sql更新语句是如何执行的)
  * [3: 事务隔离](#3-事务隔离)
  * [4:深入浅出索引上](#4深入浅出索引上)
  * [5：深入浅出索引下](#5深入浅出索引下)
  * [6:全局锁和表锁（server层实现）](#6全局锁和表锁server层实现)
  * [7：行锁](#7行锁)
  * [8：事务隔离还是不隔离的？](#8事务隔离还是不隔离的)
  * [9:普通索引和唯一索引，应该怎么选择？](#9普通索引和唯一索引应该怎么选择)
  * [10：mysql为什么有时候会选错索引？](#10mysql为什么有时候会选错索引)
  * [11：怎么给字符串加索引？](#11怎么给字符串加索引)
  * [12：为什么我的Mysql会“抖”一下？](#12为什么我的mysql会抖一下)
  * [13：为什么表数据删除一半，表文件大小不变？](#13为什么表数据删除一半表文件大小不变)


                                                Mysql实战45讲

## 1：一条sql查询语句是如何执行的？
![binaryTree](/image/1/sql查询步骤.png   "binaryTree")

mysql可以分为Server 层和存储引擎层两部分。
  Server 层涵盖mysql的大多数核心功能，以及所有的内置函数（如日期，时间，数学等），所有跨存储引擎的功能都在这一层实现，如存储过程，触发器，视图等。
  存储引擎负责数据的存储和提取。其架构模式是插件式的，支持InnoDB、MyISAM、Memory等多个存储引擎。现在最常用的是InnoDB,mysql5.5开始默认存储引擎。 
所以，建表的时候不指定存储引擎，默认就是InnoDB,当然也可以在建表的时候进行指定存储引擎engine=memory,不同的存储引擎存取方式不同，支持的功能也不同。
不同的存储引擎共用一个Server 层。

Server 层
连接器：负责与客户端建立连接、获取权限、维持和管理链接。 可以在命令行里面输入账号和密码，但是在生产服务器不要直接跟密码，可能导致密码泄露。
      连接成功后，回到权限表里面查询你拥有的权限。一旦连接成功，即使用管理员账号对权限做修改，也不会影响已经存在连接的权限。只有再新建连接才会
      使用新的权限设置。连接成功后，如果没有后续动作，这个连接处于空闲状态。可以在show processlist看到，Command列显示为Sleep就是空闲连接。
      客户端如果太长时间没动静，连接器就会自动断开连接，时间由参数wait_timeout控制，默认值是8小时。一般建议使用长连接，因为建立连接的过程是很复杂的。
      问题：使用长连接后，mysql占用内存涨的比较快？
          这是因为mysql在执行过程中临时使用的内存是管理在连接对象里面的，这些资源会在连接断开的时候才释放。如果长时间累计，会被系统强行杀掉（oom）,
          也就是mysql异常重启。
      解决方法： 1：定期断开长连接。     2：mysql5.7之后的版本，在每次执行完一个比较大的操作后，通过执行mysql_reset_connection来重新初始化
               连接资源，这个过程不需要重连和重新权限验证，但是会将连接恢复到刚刚创建完成的状态。

查询缓存：mysql拿到一个查询请求后，会先查询缓存。之前执行过的sql语句及其结果会以key-valve的形式被直接缓存在内存中。
        但是我们大多数情况下都不建议使用缓存，因为对一个表的更新，这个表上所有的查询缓存都会清空，对于更新压力大的数据库来说，查询缓存的命中率会非常低。
        除非业务表很少更新，才适合使用缓存。mysql可以根据参数query_cache_type设置为demand,关闭缓存。对于确定使用缓存的sql，可以用SQL_CACHE
        指定，select SQL_CACHE * from T where ID=10;  mysql8.0之后移除了缓存。

分析器：对SQL语句进行解析，先做词法分析，识别字符串，识别表明和字段等等，再做语法分析，分析输入的sql语句是否满足mysql语法。
       如果你的语句不对，就会收到“You have an error in your SQL syntax”的错误提醒，请关注use near之后的内容。

优化器：当表里面有多个索引的时候，决定使用那个索引；或者一个语句有多表关联join的时候，决定各个表的优化顺序。

执行器：执行sql语句，开始执行的时候，判断你对表是否有执行查询的权限；如果命中缓存，会在查询缓存返回结果的时候做权限验证。查询也会在优化器之前调用 precheck 验证权限
       如果有权限，执行器就会根据表的引擎定义，去使用这个引擎提供的接口。执行完成后，结果写入查询缓存。
       问题：引擎扫描行数跟rows_examined是相同的吗？
            执行器调用一次，在引擎内部可能扫描了多行，因此引擎扫描行数跟rows_examined并不是完全相同的。在数据库的慢查询日志中看到一个 rows_examined 的字段，
            表示这个语句执行过程中扫描了多少行。这个值就是在执行器每次调用引擎获取数据行的时候累加的。


## 2：一条sql更新语句是如何执行的？
mysql的WAL技术:全称Write-Ahead Logging,关键点就是先写日志,再写磁盘。
 redo log:相当于小黑板,更新记录一般先写到redo log里,并更新内存,这时候更新操作就算完成了.InnoDB会在适当的时候,将操作记录更新到磁盘里面.
          redo log是固定大小的,写满了就必须要更新到磁盘一部分,ring buffer.从头开始写,写到末尾又回到开头循环写.
          有了redo log(InnoDB引擎特有的日志),InnoDB就可以保证即使数据发生异常重启,之前提交的记录都不会丢失,这个能力称为crash-safe.
          是物理日志,记录的是实际操作,循环写,空间固定会用完.是顺序写,并且可以组提交.满血复活
bin log: Server层也有自己的日志,称为binlog(归档日志).因为mysql一开始并没有InnoDB引擎,自带的是MiISAM.没有crash-safe能力,binlog日志
          只能用来归档,所有引擎都可以使用.是逻辑日志,记录的是语句的原始操作,追加写,binlog文件写到一定大小后会切换到下一个,并不会覆盖以前的日志.
          binlog有两种模式,statement格式记录sql预计,row格式记录行的内容,更新前和更新后,记录两条.
          binlog可关闭,set sql_log_bin=0关掉本线程的binlog日志.制造影分身
          
问题:redo写入的过程:两阶段提交,使得日志恢复的状态和数据库状态一致.
     1:查询到数据行,先加载到内存,完成更新操作后,调用引擎接口写入这行新数据.引擎将这行新数据更新到内存中,同时将这个更新操作记录
       到redo log中,此时redo log 处于prepare状态.然后告知执行器执行完成,随时可以提交事务.
     2:执行器生成这个操作的binlog,并把binlog写入磁盘
     3:执行器调用引擎的提交事务接口,引擎把刚刚写入的redo log改成提交(commit)状态,更新完成.

数据库的扩容:需要搭建一些备库开增加系统的读能力的时候,常采用全量备份+binlog实现.假如binlog数据不一致可能会导致主从数据不一致.


## 3: 事务隔离
ACID：原子性、一致性、隔离性、持久性。
当数据库有多个事务同事执行的时候，就可能出现脏读、不可重复读、幻读的问题，为了解决这些问题，就有了隔离级别的概念。隔离的越严实，效率就越低，安全性越高。

sql标准的事务隔离级别包括：读未提交、读已提交、可重复读和串行化。oracle默认读提交,mysql默认可重复读
读未提交(ru)：一个事务还没提交时，它做的变更就能被别的事务看到。没有视图的概念，直接返回记录上的最新值,带来脏读、不可重复度、幻读
读已提交（rc）：一个事务提交之后，它做的变更才会被其他事务看到。MVCC视图会在每一个SQL语句执行前创建一个，避免脏读，带来不可重复读和幻读
可重复读(rr)：一个事务在执行过程中，读取到的数据总是跟这个事务在启动时看到的数据是一致的。MVCC视图在开始事物的时候创建，整个事务存在期间都用这个事务，避免脏读和不可重复读，带来幻读
串行化(Serial)：对于同一行记录，读加共享锁，不允许写；写会加排他锁，不允许读写，当出现读写锁冲突的时候，后访问的事务必须等到前一个事务执行完成，才能继续执行。没有视图，通过锁实现数据访问，避免所有
解决幻读：使用间隙锁，mysql会为索引维护一套B+树，将索引分割成几个区间，比如（负无穷到10】,（10,30），【30到正无穷），操作10的时候，第一个区间与第二个区间加了间隙锁
 
transaction-isolation参数设置，通过 show variables like 'transaction-isolation'查看当前隔离级别 

事务隔离的实现：通过undo log和视图read-view实现事务隔离
在mysql中，实际上每条记录在更新的是狗都会同时记录一条回滚操作在 undo日志里。记录上的最新值，通过回滚操作，都可以得到前一个状态的值。类似于git的版本迭代
同一个记录在系统中可以存在多个版本，就是数据库的多版本并发控制（MVCC）。

问题：回滚日志的删除？
     系统会判断，当没有事务再需要用到这些回滚日志时，就是系统里没有比这个回滚日志更早的read-view的时候，回滚日志会被删除。事务被提交后，undo log
     不会立即被删除，而是放入代清理的链表，由purge线程判断是否有其他事务使用undo段中上一个事务的版本信息。

问题：为什么不建议使用长事务？
    长事务意味着系统里面会存在很老的事务视图。由于这些事务随时可能访问数据库里面的任何数据，所以在这个事务提交之前，数据库里面可能用到的回滚记录
    都必须保留，会导致大量占用存储空间。在mysql5.5之前的版本，回滚日志是跟随数据字典一起放在ibdata文件里的，即使长事务最终提交，回滚段被清理，
    文件也不会变小。另外，长事务还占用锁资源，也可能拖垮整个库。

问题：事务的启动方式：事务在第一个语句时才开启
   1：显示启动事务语句，begin或者start transaction。配套的语句是commit,回滚语句是rollback
   2：set autocommit=0,这个命令会将线程的自动提交关闭掉，意味着你只执行了一个select语句，事务启动，但是并不会自动提交。知道你主动执行commit或者
     rollback语句，或者断开长连接。有些客户端框架会默认连接成功后执行一个set autocommit=0的操作，这就导致接下来的查询都在事务中，长连接导致了
     意外的长事务。因此，建议总是使用set autocommit=1,通过显式的方式来启动事务。减少交互次数，可以使用commit work and chain，少执行一个
     begin语句。
查询长事务： select * from information_schema.innodb_trx where TIME_TO_SEC(timediff(now(),trx_started))>60


## 4:深入浅出索引上
索引的出现就是为了提高数据查询的效率，就像书的目录一样。mysql中，索引是在存储引擎层实现的。
索引的常见模型：哈希表、有序数组和搜索树。
哈希表+链表：类似于hashMap1.7,适用于等值查询的场景，在范围查询走全表查询，比如Memcached及一些其他的NoSQL引擎
有序数组：在等值查询和范围查询场景中的性能都非常优秀。在中间插入需要移动，成本太高。只适用于静态存储引擎。
搜索树：多使用N叉树，N取决于数据块的大小，不使用二叉树是为了减少访问多个数据块，减少寻址时间。mysql默认一个节点的长度是16K，一个整数字段索引的长度为8B,
      另外每个索引还跟着6B的指向其子树的指针，所以16K/14B =1170，接近1200数据块。树根的数据块总是在内存中，树的第二层也有很大概率在内存中。
LSM树:跳表，在redis中使用

InnoDB的索引模型：B+树，很好的配合磁盘的读写特征，减少单次查询的磁盘访问次数。
在InnoDB 中，表都是根据主键顺序以索引的形式存放的，这种存储方式的表称为索引组织表。InnoDB使用B+树模型，每一个索引在InnoDB里都是一颗B+树。
索引分成主键索引和非主键索引。
主键索引的叶子结点存的是页，页里面有多个整行数据，也就是数据块，在InnoDB里，主键索引也被称为聚簇索引。
非主键索引的叶子节点内容是页，页里面有多个主键的值，在InnoDB里，非主键索引也被称为二级索引。
使用主键查询，一般只需要一次;而使用非主键索引是先找到主键，再到主键索引里去搜索，这个过程称为回表。所以，我们应该尽量使用主键索引。如果我们不创建主键，
InnoDB会默认创建一个Rowid做主键

自增主键防止页分裂，逻辑删除并非物理删除防止页合并。
页分裂：数据所在数据页满了，需要申请一个新的数据页，然后挪动部分数据过去。这个过程称为页分裂，影响性能和数据页的利用率。
页合并：相邻两个页由于删除了数据，利用率很低之后，会将数据页合并。
问题：有什么场景适合用业务字段直接做主键的？
     只有一个索引，而且改索引必须是唯一索引。

问题：重建索引k ,以及重建主键索引是否合理。
重建索引K是可以的，使得数据按顺序插入，页面的利用率变高，减少页分裂，省空间，合理。有些时候表的记录删除了，但是它的索引还在，并未释放。
```mysql
alter table T drop index k;
alter table T add index(k);
```
但是重建主键的过程不合理，不论是删除主键还是创建主键，都会将整个表重建。


## 5：深入浅出索引下
如何避免回表操作？
二级索引查询结果仅仅是主键，此时不需要回表查主键索引，称为覆盖索引。

联合索引：多个字段在一起建立索引。建立时考量复用能力。
B+树这种索引结构，可以利用索引的“最左原则”，来定位记录。这个最左前缀可以是联合索引的最左的N个字段，也可以是字符串索引的最左M个字符。
问题：既有联合索引，也有单独的索引，选择单独索引在小字段上。

索引下推：
mysql5.6之前先一个个回表，到主键索引上找出数据行，再对比字段值。
mysql5.6之后引入索引下推，可以在索引遍历过程中，对索引中包含的字段先做判断，直接过滤掉不满足条件的记录，减少回表次数。


## 6:全局锁和表锁（server层实现）
数据库锁设计的初衷是处理并发问题。
根据枷锁的范围，mysql里面的锁大致可以分成全局锁、表级锁和行级锁。
全局锁：对整个数据库实例加锁。mysql加全局锁：Flush tables with read lock（FTWRL），让整个库处于只读状态。
   使用场景：做全库逻辑备份，也就是把整库每张表都select出来存成文本。不加锁备份的库不是一个逻辑时间点，数据不一致
   风险：主库做备份，在备份期间都不能执行更新，业务基本上就得停摆;从库上做备份，不能及时执行主库传来的binlog，导致主从延迟。
 除了加全局锁，还可以在可重复隔离级别下开启事务来备份。而且，这个过程中数据的增删改查不受影响。

问题: 可重复性隔离级别下开启事务备份和FTWRL区别？
    前者显然比后者好，但是为什么还需要后者？因为有的引擎不支持事务和可重复读这个隔离级别。比如MyISAM不支持事务，无法使用前者。
    官方自带的逻辑备份工具是mysqldump,使用参数-single-transaction开启事务，只适用于支持事务的引擎。这也是InnoDB代替MyISAM的原因。

问题：如果发现应用程序里有lock tables这样的语句，应该怎么办？
   1：要么是系统还在用MyISAM这类不支持事务的引擎，那么安排更换引擎
   2：如果引擎已经升级，代码还没升级。只需要把 lock tables 和 unlock tables改成begin和commit即可。

问题：对于FTWRL，全库只读，为什么不是用set global readonle=true的方式？
   1：在有些系统中，readonly的值被用来做其他逻辑，比如判断一个库是主库还是从库，修改这个值的影响面太大。
   2：FTWRL加锁，客户端发生异常断开，mysql会自动释放这个全局锁；而将整个库设为readonly后，发生异常，数据库会一直保持readonly状态，导致库长时间处于不可写状态，风险较高。

表级锁：增删改查（DML），修改表结构（DDL） mysql中表级别锁分两类，一种是表锁，一种是元数据锁（MDL）
 表锁的语法是 lock table  ...read/write  ,可以用 unlock table主动释放锁，也可以在客户端断开时自动释放。
   注意：如果线程A执行了 lock table t1 read,t2 write，那么只有A线程只可以读t1,读写t2.其他线程可以读t1,不可以读写t2，同时A线程也不能访问除t1、t2之外的表。
 元数据锁（MDL）：mysql5.5引入，不需要显示使用，在访问一个表的时候会被自动加上，保证读写的正确性。
   注意：对数据做CURD时加MDL读锁，其他线程可正常对表CURD，但是同一行数据更新会互斥，互斥的是行锁，不是MDL；对表结构做修改的时候，加MDL写锁，其他线程不能执行任何操作。
 
  问题：有时候给一个小表加字段，导致整个库挂了？
      MDL导致，在一个事务中，A线程先读取数据，加了MDL读锁；B线程进来，此时A线程没有执行完，A线程修改表结构，加MDL写锁；C线程进来，既不能读，也不能写。
      此时有查询比较频繁的语句，就会被阻塞，如果客户端有重试机制，重起线程进行访问，这个库的线程很快被塞满。

  问题：如何安全的给小表加字段？
      一般的，在alter table语句里面设置等待时间，等待时间内能拿到MDL写锁最好，拿不到也不阻塞后面的业务，先放弃，之后载通过重试命令重复过程。
      MariaDB已经合并了AliSQL这个功能，目前这两个开源分支都支持下面的语句：
```mysql
alter table T nowait add columu...
alter table T wait N  add column...
```

问题：备份在备库上进行，用-single-transaction方法，另一边，在主库上对一个小表的字段做DDL,在备库会看到什么现象？
     备份的几个关键动作  Q1:开启事务   Q2:设置一个保存点  Q3:拿到表结构   Q4:正式导数据   Q5:回滚/提交，释放DML锁
     对主库表修改的操作 在Q3之前来到：没有影响，备份拿到的是DDL之后的表结构
                     在Q3-Q4间来到：Q3拿到了表结构，Q4导数据时表结构被修改，报Table definition has changed, please retry transaction，现象：mysqldump 终止；
                     在Q4-Q5间来到：mysqldump占着表的MDl读锁，binglog被阻塞  现象：主从延迟，直到Q5完成，备份拿到的是DDL之前的表
                     在Q5之后来到：mysqldump释放了读锁，    现象：没有影响，备份拿到的是DDL之前的表


## 7：行锁
mysql的行锁是在引擎层由各个引擎自己实现的，并不是所有的引擎都支持行锁，MyISAM引擎就不支持行锁，意味着同一张表同一时刻只能有一个更新在执行。
innodb行级锁是通过锁索引记录实现的，如果更新的列没建索引是会锁住整个表的。
在InnoDB中，行锁是在需要的时候才加上的，但并不是不需要了就立刻释放，而是要等到事务结束时才释放。这个就是两阶段锁协议。尽量将并发度高的行，放在事务的最后执行。

死锁：并发系统中不同线程出险循环资源依赖，设计的线程都在等待别的线程释放资源时，就会导致这几个线程都进入无限等待的状态，称为死锁。
两种解决方法：
   1：直接进入等待，直至超时，这个超时时间是可以通过参数innodb_lock_wait_timeout来设置。默认50s,超时退出对于在线服务这个时间显然无法接受，但是设置的很小又无法判断是不是简单的锁等待
   2：发起死锁检测，发现死锁后，主动回滚链条中的某一个事务，让其他事务得以运行。将参数innodb_deadlock_detect设置为on,开启逻辑，这个是默认开启的
      不过这个是可以发现死锁并进行处理的，但是有额外负担，需要判断所在线程的依赖线程有没有被别人锁住，如此循环.如果有1000个并发线程,那么这个死锁
      检测操所就是100万级别的,虽然最终检测没有死锁,但是这期间要消耗大量的cpu资源，因此CPU利用率很高。

问题：活动，100个事务，CPU消耗近100%,mysql挂掉,如何解决（热点数据行更新导致的性能问题）？
    1：头痛医头的方法：如果确保业务一定不会死锁，可以临时把死锁检测关闭。但是存在风险，一旦死锁就会出现大量的超时。
    2：控制并发度。在代码端控制，做限流，对于同一行的操作不要来太多线程请求。这样死锁检测的工作量就不会很大
    3：拆行，一行拆多行，总和等于所有行记录的总值。每次操作时，可以随机选择行操作，降低冲突概率，减少锁等待个数， 减少死锁检测的cpu消耗。
       但是对于减少操作，就需要判断一部分记录行变成0的情况，代码要特殊处理。


## 8：事务隔离还是不隔离的？
begin /start transaction 命令并不是一个事务的起点，在执行到它们之后的第一个操作InnoDB表的语句，事务才真正启动。一致性视图在执行第一个快照读语句创建。
如果想立刻启动一个事务，可以用 start transaction with consistent snapshot 命令。一致性视图立即创建。

mysql中，有两个视图的概念
一个是view：用查询语句定义出来的虚拟表，创建语法是 create view ... ，查询方法和表一样。
一个是InnoDB在实现MVCC时用到的一致性视图，即 consistent read view，用于支持RC（读提交）和RR（可重复读）隔离级别的实现。没有物理结构，作用是事务执行期间用来定义能看到什么数据。

问题：快照在MVCC里是怎么工作的？
在RR级别下，事务启动的时候就拍了一个快照，基于整库的。InnoDB里面每个事务都有一个唯一的事务ID，叫做transaction id。是在事务开启的时候向InnoDB
的事务系统申请的，是按申请顺序严格递增的。每行数据都是有多个版本的，每次事务更新数据的时候，都会生成一个新的数据版本，并且把事务id复制给这个数据版本，
记作 row_trx_id。同时，旧的版本数据要保留，并且在新的数据版本中，能够有信息可以直接拿到它。也就是说，数据表中的一行数据，其实可能有多个版本，每个
版本都有自己的row_trx_id。
InnoDB为每个事务创造了一个数组，用来保存这个事务启动瞬间，正在活跃（启动了但是未提交）的事务id。数组里事务ID的最小值几位低水位，当前系统里面已经创建
过的事务ID的最大值+1记为高水位。这个视图和高水位，就组成了当前事务的一致性视图。
                【低水位  高水位（当前事务】   
已提交事务（可见）            未提交事务（不可见）              未开启事务（不可见）

更新逻辑
对于更想语句，如果按照一致性读，是不对的。更新数据都是先读后写的，这个读，是当前读。
除了update外，select语句如果加了锁，也是当前读。比如 lock in share mode(读锁，S锁，共享锁)，或者 for update(写锁，X锁，排他锁)

每个事务或者语句都有自己的一致性视图。普通查询语句是一致性读，一致性读会根据row_trx_id和一致性视图确定数据版本的可见性。
对于可重复读，查询只承认在事务启动前就已经提交完成的数据。
对于读提交，查询只承认语句启动前就已经提交完成的数据。
而当前读，总是读取已经提交完成的最新版本。


## 9:普通索引和唯一索引，应该怎么选择？
引擎的数据是按页为单位来读写的。当需要读一条记录的时候，并不是将这个记录本身从磁盘读出来，而是以页为单位，将其整体读入内存。在InnoDB中，每个数据页的大小默认是16KB。
数据页内部通过二分法来定位记录，数据页之间通过双向链表串接，

查询过程：对于普通索引，查询所有记录；对于唯一索引，查询到满足条件的一个之后停止。 性能微乎其微差别。
更新过程：当需要更新一个数据页的时候，如果数据页在内存中，直接更新；如果数据页不在内存中，在不影响数据一致性的前提下，InnoDB会将这些更新操作缓存在 
         change buffer中，就不需要从磁盘中读取数据页了。在下次查询需要访问这个数据页的时候，将数据页读入内存，然后执行change buffer与这个页
         相关的操作。将更新操作记录在change buffer，减少读磁盘，语句执行数独会得到提升，而且，数据渡读入内存是需要占用buffer pool的，这种方式
         还可以避免占用内存，提高内存利用率。
   change buffer:用的是buffer pool里的内存，因此不能无限增大。change buffer的大小可以通过参数innodb_change_buffer_max_size 来动态设置。
                 当这个参数设置成50的时候，表示change buffer的大小最多只能占用buffer pool的50%.
   merge：将change buffer中的操作应用到原数据页，得到最新结果的过程称为merge。除了访问这个数据页会触发merge外，系统有后台线程会定期merge。
          在数据库正常关闭的过程中，也会执行merge操作。
注意：redo 不仅会记录更新页在内存中的更新操作，也会记录change buffer的一些操作。redo log主要节省的是随机写磁盘的IO操作（转成顺序写），而change
     buffer主要节省的是随机读磁盘的IO消耗。

问题：change buffer的使用场景
    只限于普通索引，而不适用于唯一索引，而且尽量把change buffer开大。且使用于更新操作多，查询少的场景，业务模型常见的有账单类，日志类的系统。


## 10：mysql为什么有时候会选错索引？
 不断的删除历史数据和新增数据可能会导致后面查询时选错所引。

选择索引是优化器的工作。优化器选择索引的目的，是找到一个最优的方案，并用最小的代价去执行语句。
在数据库里，扫描行数是影响执行代价的因素之一。扫描的行数越少，意味着访问磁盘数据的次数越少，消耗的CPU资源越少。 扫描行数+回表扫描行数 和 全表扫描进行对比
当然，扫描行数不是唯一的判断标准，优化器还会结合是否使用临时表、是否排序等因素进行综合判断。

当不存在临时表和排序的时候，mysql选错索引肯定是判断扫描行数的时候出问题了。
mysql在真正执行语句之前，并不能精确的知道满足这个条件的记录有多少，而只能根据统计信息来估算记录数。这个统计信息就是索引的“区分度”。
显然，一个索引上不同的值越多，这个索引的区分度就越好。而一个索引上不同的值的个数，我们称之为“基数”，基数越大，索引的分区度就越好。
可以使用 show index方法，看到一个索引的基数。
```mysql
show index from t;
```
问题：mysql怎样得到索引的基数？
通过”采样统计“。InnoDB会默认选择N个数据页，统计这携页面上的不同值，得到一个平均值，然后乘以这个索引的页面数，得到这个索引的基数。
数据表是持续更新的，索引统计信息也不会固定不变。所以，当变更的行数超过 1/M 的时候，会自动触发重新做一次索引统计。
有两种存储索引统计的方式，可以通过设置参数 innodb_stats_persistent 的值来选择，有两种存储索引统计的方式，可以通过设置参数 innodb_stats_persistent 的值来选择：
设置为 on 的时候，表示统计信息会持久化存储。这时，默认的 N 是 20，M 是 10。
设置为 off 的时候，表示统计信息只存储在内存中。这时，默认的 N 是 8，M 是 16。
由于是采样统计，所以不管 N 是 20 还是 8，这个基数都是很容易不准的。

问题： MySQL选错索引怎么破？
1：强制指定使用某个索引，不常用不推荐用
```mysql
explain select * from T where a between 1000 and 2000;
#强制使用a索引
explain select * from T force index（a） where a between 1000 and 2000;
```
2：调整SQL语句，使优化器选择的和我们想的一样，不具有通用性
```mysql
#下面这条语句会默认使用b索引，在我们看来，使用a扫描1000行，使用b扫描50000行，应该是a扫描行数少才对。 但是经过执行，系统选择了b索引
#显然，扫描行数的估计值依然不不准确，mysql又选错了索引。为什么选择索引b ,是因为 order by b,优化器认为使用索引b可以避免排序，索引已有序，几十扫描行数多，也判定为代价小
explain select * from T where a between 1 and 1000 and b between 50000 and 100000 order by  b limit 1;
#调整sql语句.order by b,a    意味着使用那个都需要排序，因此，扫描行数成了影响决策的主要条件。
explain select * from T where a between 1 and 1000 and b between 50000 and 100000 order by  b,a limit 1;
```
3：新建更合适的索引或者删除不合适的索引，是一个思路
```mysql
# 新增索引的情况比较少，找到一个合适的索引是很难的。  删除不合适的索引倒是可以考虑
```
4：使用analyze table可以解决索引统计信息不准确导致的索引选错的问题
```mysql
#使用命令，重新统计索引信息，避免得到错误的扫面行数。
analyze  table T;
explain select * from T where a between 1000 and 2000;
```


## 11：怎么给字符串加索引？
前缀索引：定义字符串的一部分作为索引。占用空间小，但是可能会增加额外的记录扫描次数,会影响性能。所以，定义好长度很关键。
默认的，如果创建索引的语句不指定前缀长度，那么索引就会包含整个字符串。
前缀索引越长，回表查询次数就越少；前缀索引越长，占用磁盘空间就越大，相同的数据页能放下的索引值就越少，搜索的效率就会降低。

问题：有什么方法能够确定我们应该使用多长的前缀呢？
    简历索引关心的是区分度，区分度越高越好，意味着重复的键值越少。
```mysql
# 算出列上有多少个不同的值
select count(distinct email) as L form T;

# 选取不同长度的前缀来看这个值
select 
       count (distinct left(email,4)) as L4 ,
       count (distinct left(email,5)) as L5 ,
       count (distinct left(email,6)) as L6 ,
       count (distinct left(email,7)) as L7 
from T
```
注意：使用前缀索引很可能会损失区分度。所以你需要预先设定一个可以接受的损失比例，比如5%.

问题：前缀索引对覆盖索引的影响？
   使用前缀索引，需要回到ID索引，去进行回表。而使用覆盖索引的话不需要进行回表了。 所以：前缀索引会导致覆盖索引不可用。

问题：对于长字符串，应该怎样进行进行建立索引？
1：直接创建完整索引，这样可能会比较占用空间
2：建立前缀索引，节省空间，但是会增加扫描次数，并且不能使用覆盖索引
3：字符串到序存储+前缀索引,增大区分度,解决字符串本身前缀的区分度不够的问题，不支持范围查询
```mysql
# 倒序存储的查询方式
select  field_first_list from T where id_card =reverse('input_id_card_string');
```
4：添加hash字段+在hash字段上加索引，查询性能稳定，有额外的存储和计算消耗，不支持范围查询
```mysql
# 在表上再创建一个整数字段，来保存字段的检验码，同时在这个字段上创建索引
alter table T add id_card_crc int unsigned,add index(id_card_crc)

# 每次插入新记录的时候，都同时使用crc32（）函数得到校验码填到这个新字段。但是检验码可能存在冲突，还要还要判断字段是否相同
select field_list from T where id_card_crc =crc32('input_id_card_string') and id_card='input_id_card_string';
```
5：字段拆分（一个字段可拆为两个以上）


## 12：为什么我的Mysql会“抖”一下？
一条sql语句，正常执行的时候特别快，但有时候特别慢，并且这样的场景很难复现，并不随机，而且持续时间还很短。看上去，就像是数据库“抖”了一下一样。

一条sql更新操作-->先更新内存-->写 redo log--->等待空闲时间写入磁盘（更新账本）  flush操作
内存数据页跟磁盘数据页内容不一致的时候，我们称这个内存为 ‘脏页’。
内存写入磁盘后，内存和磁盘上的数据页的内容就一致了，称为 ‘干净页’。
不论是脏页还是干净页，都在内存中。

mysql抖一下，就是在刷脏页。刷脏页的场景：
1：redo log满了：整个系统不能再接受更新，所有的更新都必须堵住，这种情况是要避免的
2：内存满了：需要淘汰一些数据页，空出内存给别的数据页使用。
           InnoDB用缓冲池管理内存，缓冲池中的内存页有三种状态：1：还没有使用的   2：使用了并且是干净页   3：使用了并且是脏页
           InnoDB的策略是尽量使用内存。
3：mysql空闲的时候
4：mysql正常关闭的时候

刷脏页是常态，但是出现以下两种情况，都是会明显影响性能的：
1：一个查询要淘汰的脏页个数太多，会导致查询的响应时间明显变长
2：redo log写满，更新全部堵住，写性能跌为0,这种情况对敏感业务来说，是不能接受的。
所以，InnoDB 需要有控制脏页比例的机制，来尽量避免上面的情况。

问题：InnoDB 刷脏页的控制策略
1：告知InnoDB所在主机的IO能力，这样InnoDB才能知道需要权利刷脏页的时候，可以刷多快， innodb_io_capacity参数，告诉InnoDB你的刷盘能力。
   innodb_io_capacity参数，建议设置成磁盘的IOPS。
  太大了，会导致全力刷盘，负荷太大;太小了，无法利用主机的性能。
  那么如何设置？
主要有两个因素：一个是脏页比例，一个是redo lod写盘速度
 参数 innodb_max_dirty_pages_pct 是脏页比例上限，默认是75%.,InnoDB 会根据当前的脏页比例（假设为M），算出一个0-100之间的数字，计算公式记为 F1(M)
 InnoDB每次写入的日志都有一个序号（LSN），当前写入的序号与chenkpoint之间的差值，假设为N。InnoDB会根据N计算出一个0-100之间的数字，计算公式记为F2(N),N越大，计算出来的值越大
根据上述计算出来的 F1(M) 和 F2(N) ,取其中的较大值记为 R，之后innoDB 就可以根据 innodb_io_capacity 乘以 R% 来控制刷脏页的速度。
![binaryTree](/image/12/刷脏页的速度.png  "binaryTree")
 要尽量避免这种状况，合理的设置innodb_io_capacity的值，并且平时要多关注脏页比例，不要让它经常接近75%

问题：刷脏页的连坐问题
淘汰数据页的时候会连坐，刷脏页的时候，发现邻居也是脏页，也会一并刷了，这个行为会不断传递下去。
在InnoDB中,参数innodb_flush_neighbors用来控制这个行为。值为1的时候会连坐，值为0，表示不找邻居，自己刷自己的。
这个找邻居优化机制在硬盘的时代是很有意义的，可以减少很多随机IO，机械硬盘的随机IOPS一般只有几百，相同的逻辑操作减少随机IO就意味着系统性能的大幅度提升。
如果使用的是SSD这种比较高的设备的话，建议关闭连坐，这时候IOPS往往不是瓶颈，而只刷自己，就可以更快的完成刷脏页操作，减少SQL语句相应时间。
在mysql 8.0中，innodb_flush_neighbors 参数的默认值就已经是0了。

问题：innoDB如何知道一个数据页是不是脏页？
 每个数据页又不有LSN，8字节，每次修改都会变大。对比这个LSN 和checkpoint的 LSN，比checkpoint小的是干净页。


## 13：为什么表数据删除一半，表文件大小不变？















